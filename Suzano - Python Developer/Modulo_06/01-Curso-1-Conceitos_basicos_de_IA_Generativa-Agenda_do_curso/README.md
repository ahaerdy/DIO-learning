- Instrutor:
- Contato Linkedin: 

## 🟩 Vídeo 01 - Conceitos básicos de IA Generativa - Agenda do curso

### Agenda

- [ ] Conceitos básicos de IA generativa  
- [ ] Conceitos básicos do Serviço OpenAI do Azure  
- [ ] Explore a IA Generativa Responsável

### Objetivos de Aprendizado

- Descreva a IA generativa.  
- Descreva os recursos de grandes modelos de linguagem.  
- Entenda como usar o Azure OpenAI para criar soluções generativas de IA.


## 🟩 Vídeo 02 - O que é IA Generativa

### O que é IA generativa?

IA: imita o comportamento humano usando aprendizado de máquina para interagir com o ambiente e executar tarefas sem instruções explícitas sobre o que gerar.

### IA generativa

IA generativa: cria conteúdo original, como IA gerativa que foi incorporada a aplicativos de chat.  
Os aplicativos de IA gerativa usam entrada em linguagem natural e retornam respostas apropriadas em uma variedade de formatos:

## 🟩 Vídeo 03 - Modelos de linguagem grandes

### Modelos de linguagem grandes

Os aplicativos de IA generativa são alimentados por LLMs (modelos de linguagem grandes),  
que são um tipo especializado de modelo de machine learning que você pode usar para executar tarefas de PLN (processamento de linguagem natural), incluindo:

### Tarefas de PLN com Modelos de Linguagem Grandes

- [ ] Determinar sentimento ou classificar de outra forma o texto em idioma natural.  
- [ ] Resumir um texto.  
- [ ] Comparar várias fontes de texto quanto à similaridade semântica.  
- [ ] Geração de nova linguagem natural.


## 🟩 Vídeo 04 - Modelos de linguagem grandes: Transformador parte 1

### Modelos de linguagem grandes - transformador

A arquitetura do modelo do transformador consiste em dois componentes principais, ou blocos.

### Componentes da Arquitetura Transformadora

- Um bloco *codificador* que cria representações semânticas do vocabulário de treinamento.  
- Um bloco *decodificador* que gera novas sequências de linguagem.


## 🟩 Vídeo 05 - Modelos de linguagem grandes: Transformador parte2

### Modelos de linguagem grandes - transformador

- O texto é tokenizado para que cada palavra ou frase seja representada por um token numérico exclusivo.  
- Inserções (valores de vetor com várias dimensões) são atribuídas aos tokens.

### Mecanismo de Atenção

- ☐ As camadas de atenção examinam cada token por vez e determinam valores incorporados que refletem os relacionamentos semânticos entre os tokens.

### Decodificador

- No decodificador, essas relações são usadas para prever a sequência mais provável de tokens.

<details>
<summary> Slide da aula🔻</summary>
<p align="center">
    <img src="images/image.png" alt="" width="840">
</p>
</details>


## 🟩 Vídeo 06 - Modelos de linguagem grande: Tokenização

### Modelos de linguagem grandes - tokenização

#### Etapa um: tokenização

- A primeira etapa no treinamento de um modelo de transformador é decompor o texto de treinamento em tokens.

**Frase de exemplo:** *Eu ouvi um cachorro latir alto para um gato.*

<details>
<summary> Slide da aula🔻</summary>
<p align="center">
    <img src="images/image-2.png" alt="" width="840">
</p>
</details>

### Representação por Tokens

- A frase agora é representada com os tokens: [1 2 3 4 5 6 7 3 8].
- Observe que “um” é tokenizado como 3 apenas uma vez.
- Da mesma forma, a frase “Eu ouvi um gato” poderia ser representada com as fichas [1 2 3 8].


### Etapa dois: inserções

- ☐ As relações entre tokens são capturadas como vetores, conhecidos como inserções.

## 🟩 Vídeo 07 - Modelos de linguagem grandes: Inserções

### Etapa dois: inserções

☐ As relações entre tokens são capturadas como vetores, conhecidos como inserções.

<p align="center">
    <img src="images/image-3.png" alt="" width="840">
</p>

#### que isso representa?
- Proximidade semântica: Palavras com significados semelhantes (como “Cachorro” e “Latir”) estão próximas no espaço vetorial.
-Representação matemática de linguagem: Ao transformar palavras em vetores, modelos de PLN conseguem realizar tarefas como tradução, classificação de texto, geração de texto, etc.
-Contextualização: A imagem parece ser uma continuação da etapa anterior (“Etapa dois: inserções”), ilustrando como as relações entre tokens são capturadas visualmente.

## 🟩 Vídeo 08 - 

## 🟩 Vídeo 09 - 

## 🟩 Vídeo 10 - 

## 🟩 Vídeo 11 - 

## Certificado