- Instrutor:
- Contato Linkedin: 

## ğŸŸ© VÃ­deo 01 - Conceitos bÃ¡sicos de IA Generativa - Agenda do curso

### Agenda

- [ ] Conceitos bÃ¡sicos de IA generativa  
- [ ] Conceitos bÃ¡sicos do ServiÃ§o OpenAI do Azure  
- [ ] Explore a IA Generativa ResponsÃ¡vel

### Objetivos de Aprendizado

- Descreva a IA generativa.  
- Descreva os recursos de grandes modelos de linguagem.  
- Entenda como usar o Azure OpenAI para criar soluÃ§Ãµes generativas de IA.


## ğŸŸ© VÃ­deo 02 - O que Ã© IA Generativa

### O que Ã© IA generativa?

IA: imita o comportamento humano usando aprendizado de mÃ¡quina para interagir com o ambiente e executar tarefas sem instruÃ§Ãµes explÃ­citas sobre o que gerar.

### IA generativa

IA generativa: cria conteÃºdo original, como IA gerativa que foi incorporada a aplicativos de chat.  
Os aplicativos de IA gerativa usam entrada em linguagem natural e retornam respostas apropriadas em uma variedade de formatos:

## ğŸŸ© VÃ­deo 03 - Modelos de linguagem grandes

### Modelos de linguagem grandes

Os aplicativos de IA generativa sÃ£o alimentados por LLMs (modelos de linguagem grandes),  
que sÃ£o um tipo especializado de modelo de machine learning que vocÃª pode usar para executar tarefas de PLN (processamento de linguagem natural), incluindo:

### Tarefas de PLN com Modelos de Linguagem Grandes

- [ ] Determinar sentimento ou classificar de outra forma o texto em idioma natural.  
- [ ] Resumir um texto.  
- [ ] Comparar vÃ¡rias fontes de texto quanto Ã  similaridade semÃ¢ntica.  
- [ ] GeraÃ§Ã£o de nova linguagem natural.


## ğŸŸ© VÃ­deo 04 - Modelos de linguagem grandes: Transformador parte 1

### Modelos de linguagem grandes - transformador

A arquitetura do modelo do transformador consiste em dois componentes principais, ou blocos.

### Componentes da Arquitetura Transformadora

- Um bloco *codificador* que cria representaÃ§Ãµes semÃ¢nticas do vocabulÃ¡rio de treinamento.  
- Um bloco *decodificador* que gera novas sequÃªncias de linguagem.


## ğŸŸ© VÃ­deo 05 - Modelos de linguagem grandes: Transformador parte2

### Modelos de linguagem grandes - transformador

- O texto Ã© tokenizado para que cada palavra ou frase seja representada por um token numÃ©rico exclusivo.  
- InserÃ§Ãµes (valores de vetor com vÃ¡rias dimensÃµes) sÃ£o atribuÃ­das aos tokens.

### Mecanismo de AtenÃ§Ã£o

- â˜ As camadas de atenÃ§Ã£o examinam cada token por vez e determinam valores incorporados que refletem os relacionamentos semÃ¢nticos entre os tokens.

### Decodificador

- No decodificador, essas relaÃ§Ãµes sÃ£o usadas para prever a sequÃªncia mais provÃ¡vel de tokens.

<details>
<summary> Slide da aulağŸ”»</summary>
<p align="center">
    <img src="images/image.png" alt="" width="840">
</p>
</details>


## ğŸŸ© VÃ­deo 06 - Modelos de linguagem grande: TokenizaÃ§Ã£o

### Modelos de linguagem grandes - tokenizaÃ§Ã£o

#### Etapa um: tokenizaÃ§Ã£o

- A primeira etapa no treinamento de um modelo de transformador Ã© decompor o texto de treinamento em tokens.

**Frase de exemplo:** *Eu ouvi um cachorro latir alto para um gato.*

<details>
<summary> Slide da aulağŸ”»</summary>
<p align="center">
    <img src="images/image-2.png" alt="" width="840">
</p>
</details>

### RepresentaÃ§Ã£o por Tokens

- A frase agora Ã© representada com os tokens: [1 2 3 4 5 6 7 3 8].
- Observe que â€œumâ€ Ã© tokenizado como 3 apenas uma vez.
- Da mesma forma, a frase â€œEu ouvi um gatoâ€ poderia ser representada com as fichas [1 2 3 8].


### Etapa dois: inserÃ§Ãµes

- â˜ As relaÃ§Ãµes entre tokens sÃ£o capturadas como vetores, conhecidos como inserÃ§Ãµes.

## ğŸŸ© VÃ­deo 07 - Modelos de linguagem grandes: InserÃ§Ãµes

### Etapa dois: inserÃ§Ãµes

â˜ As relaÃ§Ãµes entre tokens sÃ£o capturadas como vetores, conhecidos como inserÃ§Ãµes.

<p align="center">
    <img src="images/image-3.png" alt="" width="840">
</p>

#### que isso representa?
- Proximidade semÃ¢ntica: Palavras com significados semelhantes (como â€œCachorroâ€ e â€œLatirâ€) estÃ£o prÃ³ximas no espaÃ§o vetorial.
-RepresentaÃ§Ã£o matemÃ¡tica de linguagem: Ao transformar palavras em vetores, modelos de PLN conseguem realizar tarefas como traduÃ§Ã£o, classificaÃ§Ã£o de texto, geraÃ§Ã£o de texto, etc.
-ContextualizaÃ§Ã£o: A imagem parece ser uma continuaÃ§Ã£o da etapa anterior (â€œEtapa dois: inserÃ§Ãµesâ€), ilustrando como as relaÃ§Ãµes entre tokens sÃ£o capturadas visualmente.

## ğŸŸ© VÃ­deo 08 - 

## ğŸŸ© VÃ­deo 09 - 

## ğŸŸ© VÃ­deo 10 - 

## ğŸŸ© VÃ­deo 11 - 

## Certificado